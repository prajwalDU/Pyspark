{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlKMC4jP/rDAzJ3lxMj5w8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwalDU/Pyspark/blob/main/Hadoop%26Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is spark context and spark sessions?\n",
        "Spark session and spark context is not the dataframe.\n",
        "\n",
        "It is a kine of entry gate, to access the application which we are writting.\n",
        "\n",
        "It is the begining of the programming or spark job. Where we actually create consessions, that session will create multiple context, it can be spark context, Hive context, it can be SQL context and inside that perticuler session it will run whatever transformation or actions we writing."
      ],
      "metadata": {
        "id": "ZQBD_rAIdAp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Big Data?\n",
        "Huge amount of data, which we cammot process using our traditional method. That is called Big Data."
      ],
      "metadata": {
        "id": "WupIEYMfC5bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDMS** is only soluation for transactional managment system.\n",
        "\n",
        "Let's we order in flipkar/amazon our order complete by oracal(or any transcational system).RDBs is only one who can access the property.\n",
        "\n",
        "Transactional systems are handeld by RDMS system."
      ],
      "metadata": {
        "id": "6fm3gfckD1zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JSON - Java Script Object Notation."
      ],
      "metadata": {
        "id": "jxUWEggGI4g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Warehouse\n",
        "It take data from multiple source like company data, atking data from 3rd party. this all data are stored in data warehouse, this process is done by using some ETL techniques.\n",
        "\n",
        "Data Warehouse is also called as OLAP(Online Analytical Faltform)."
      ],
      "metadata": {
        "id": "ab60beL5KpAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional architecture - It takes data from multiple source in multiple format, and trasfer it into data warehouse system to store the data using some ETL techniques.\n"
      ],
      "metadata": {
        "id": "jrXp20WmTidd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL tools are \n",
        "\n",
        "1 Informatica PowerCenter. Informatica PowerCenter is one of the best ETL tools on the market.\n",
        "\n",
        "2 Apache Airflow.\n",
        "\n",
        "3 IBM Infosphere Datastage.\n",
        "\n",
        "4 Oracle Data Integrator.\n",
        "\n",
        "5 Microsoft SQL Server Integration Services (SSIS) \n",
        "\n",
        "6 Talend Open Studio (TOS)\n",
        "\n",
        "7 Pentaho Data Integration (PDI) \n",
        "\n",
        "8 Hadoop."
      ],
      "metadata": {
        "id": "vMDJv1XKUX0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to big data company fail to respose fastly.\n",
        "\n",
        "Let's say SBI company has stored multiple data different source and multiple data type in there warehouse. let's say we withdraw money in an ATM, amount of 1000, but in some times you don't get message from bank to phone number, might me you recieve it in next day or some time. This is the main issue company are facing."
      ],
      "metadata": {
        "id": "vPzh5RsrWF-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MPP - Masive Parallel Processing."
      ],
      "metadata": {
        "id": "TI6jF7FnXG2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop is avaliable in apache website for free,\n",
        "\n",
        "If we want get paid version we can take it form \"Cloudera\", \"MAPR\" and \"Hortonworks\", Cloudera sold all tools like hadoop, spark, etc."
      ],
      "metadata": {
        "id": "EQrkYtnbZiVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop is a Framework not an software."
      ],
      "metadata": {
        "id": "fTLLfUIpdH1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1000 tera byte is 1 petabyte."
      ],
      "metadata": {
        "id": "JcJnydk8d-qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commudiative Hardware - Assemable servers."
      ],
      "metadata": {
        "id": "NZbKQKcTeVUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Hadoop mainly three components\n",
        "1. HDFS -> Stroage system.(Hadoop Distributed File System)\n",
        "2. YARN -> Resource Manager.(Y\n",
        "3. MapReduce -> Processing."
      ],
      "metadata": {
        "id": "5exnXPa3nE2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Master process running a machine called \"Name Node\".\n",
        "\n",
        "In Sleeve process running a machine called \"Data Node\"."
      ],
      "metadata": {
        "id": "iHqvz6BrPumc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A SequenceFile is a flat, binary file type that serves as a container for data to be used in Apache Hadoop distributed computing projects. SequenceFiles are used extensively with MapReduce."
      ],
      "metadata": {
        "id": "vfWdQLYnnwg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In HDFS once we stored the data, editing is not possible. that's mean modification not possible."
      ],
      "metadata": {
        "id": "hK5KPBrdQs-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of the name node is to store Meta data or the index.\n",
        "\n",
        "If Name crashaes we don't get acess to clusters. Becuase name node will gone every things will gone.\n",
        "\n",
        "**WAN DISKO** is the company helps to get disaster data back.\n",
        "\n",
        "Disc Pc (Distributed Copy) it syncronis two back-ups, primary back-ups.\n",
        "\n",
        "We want to prevent our Master Node so \n",
        "1. Active Name Node\n",
        "2. Stand by Name Node"
      ],
      "metadata": {
        "id": "0y6-VmwsV0z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is transformation"
      ],
      "metadata": {
        "id": "ZhwXgU6UdAms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is action"
      ],
      "metadata": {
        "id": "mZfELSgwdAj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How spark dag works"
      ],
      "metadata": {
        "id": "bq0DlMdadAhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what is prasintent or castry in spark"
      ],
      "metadata": {
        "id": "ISg8BPsrdAeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to read CSV file in spark"
      ],
      "metadata": {
        "id": "brnBcmxLdAbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join query in pyspark"
      ],
      "metadata": {
        "id": "cLpGJc3udAYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to join two tables or two dataset "
      ],
      "metadata": {
        "id": "a50HMVnSjUZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark architecture"
      ],
      "metadata": {
        "id": "JNX6Ze48jbLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SQL**"
      ],
      "metadata": {
        "id": "BGPdYybFjbIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window function"
      ],
      "metadata": {
        "id": "2gDdPiUhjbFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-queries"
      ],
      "metadata": {
        "id": "pJvCzrC3jbCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self join"
      ],
      "metadata": {
        "id": "pvV0CgRWmVAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partitions on different window functions"
      ],
      "metadata": {
        "id": "AjOPMuv4lxbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross join"
      ],
      "metadata": {
        "id": "vkm1sdLymr1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between spark and pyspark"
      ],
      "metadata": {
        "id": "0XM28oFFmrzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gPoLqH0-mrv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hDPI95TJmrtO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAQQcSvmmc5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}